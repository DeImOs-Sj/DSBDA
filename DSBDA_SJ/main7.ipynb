{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49d9f0f-738b-4acc-83b3-6fd5e5de39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document = \"Tokenization is the process of breaking down text into words and punctuation. POS tagging identifies the grammatical parts of speech of each word. Stop words are common words that are often removed. Stemming reduces words to their base or root form. Lemmatization is similar to stemming but aims to return the base or dictionary form of a word.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0ac167-1355-44e8-95b5-7376fe19df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'words', 'and', 'punctuation', '.', 'POS', 'tagging', 'identifies', 'the', 'grammatical', 'parts', 'of', 'speech', 'of', 'each', 'word', '.', 'Stop', 'words', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', '.', 'Stemming', 'reduces', 'words', 'to', 'their', 'base', 'or', 'root', 'form', '.', 'Lemmatization', 'is', 'similar', 'to', 'stemming', 'but', 'aims', 'to', 'return', 'the', 'base', 'or', 'dictionary', 'form', 'of', 'a', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sample_document)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5112d262-35ab-409f-9641-373a270af167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('breaking', 'VBG'), ('down', 'RP'), ('text', 'RB'), ('into', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('punctuation', 'NN'), ('.', '.'), ('POS', 'NNP'), ('tagging', 'VBG'), ('identifies', 'NNS'), ('the', 'DT'), ('grammatical', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('of', 'IN'), ('each', 'DT'), ('word', 'NN'), ('.', '.'), ('Stop', 'VB'), ('words', 'NNS'), ('are', 'VBP'), ('common', 'JJ'), ('words', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('often', 'RB'), ('removed', 'VBN'), ('.', '.'), ('Stemming', 'VBG'), ('reduces', 'NNS'), ('words', 'NNS'), ('to', 'TO'), ('their', 'PRP$'), ('base', 'NN'), ('or', 'CC'), ('root', 'NN'), ('form', 'NN'), ('.', '.'), ('Lemmatization', 'NNP'), ('is', 'VBZ'), ('similar', 'JJ'), ('to', 'TO'), ('stemming', 'VBG'), ('but', 'CC'), ('aims', 'VBZ'), ('to', 'TO'), ('return', 'VB'), ('the', 'DT'), ('base', 'NN'), ('or', 'CC'), ('dictionary', 'JJ'), ('form', 'NN'), ('of', 'IN'), ('a', 'DT'), ('word', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab03cfa-5571-4f5f-bf9f-d39c2664aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'process', 'breaking', 'text', 'words', 'punctuation', '.', 'POS', 'tagging', 'identifies', 'grammatical', 'parts', 'speech', 'word', '.', 'Stop', 'words', 'common', 'words', 'often', 'removed', '.', 'Stemming', 'reduces', 'words', 'base', 'root', 'form', '.', 'Lemmatization', 'similar', 'stemming', 'aims', 'return', 'base', 'dictionary', 'form', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a5c549-337b-41db-b56b-5b810034ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'process', 'break', 'text', 'word', 'punctuat', '.', 'po', 'tag', 'identifi', 'grammat', 'part', 'speech', 'word', '.', 'stop', 'word', 'common', 'word', 'often', 'remov', '.', 'stem', 'reduc', 'word', 'base', 'root', 'form', '.', 'lemmat', 'similar', 'stem', 'aim', 'return', 'base', 'dictionari', 'form', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adae4bf2-cb5f-431e-bee4-e803736e0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'process', 'breaking', 'text', 'word', 'punctuation', '.', 'POS', 'tagging', 'identifies', 'grammatical', 'part', 'speech', 'word', '.', 'Stop', 'word', 'common', 'word', 'often', 'removed', '.', 'Stemming', 'reduces', 'word', 'base', 'root', 'form', '.', 'Lemmatization', 'similar', 'stemming', 'aim', 'return', 'base', 'dictionary', 'form', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926dc63b-bac5-48a6-b405-fb50ce08aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['aims' 'and' 'are' 'base' 'breaking' 'but' 'common' 'dictionary' 'down'\n",
      " 'each' 'form' 'grammatical' 'identifies' 'into' 'is' 'lemmatization' 'of'\n",
      " 'often' 'or' 'parts' 'pos' 'process' 'punctuation' 'reduces' 'removed'\n",
      " 'return' 'root' 'similar' 'speech' 'stemming' 'stop' 'tagging' 'text'\n",
      " 'that' 'the' 'their' 'to' 'tokenization' 'word' 'words']\n",
      "  (0, 7)\t0.09667364890456635\n",
      "  (0, 25)\t0.09667364890456635\n",
      "  (0, 0)\t0.09667364890456635\n",
      "  (0, 5)\t0.09667364890456635\n",
      "  (0, 27)\t0.09667364890456635\n",
      "  (0, 15)\t0.09667364890456635\n",
      "  (0, 10)\t0.1933472978091327\n",
      "  (0, 26)\t0.09667364890456635\n",
      "  (0, 18)\t0.1933472978091327\n",
      "  (0, 3)\t0.1933472978091327\n",
      "  (0, 35)\t0.09667364890456635\n",
      "  (0, 36)\t0.29002094671369905\n",
      "  (0, 23)\t0.09667364890456635\n",
      "  (0, 29)\t0.1933472978091327\n",
      "  (0, 24)\t0.09667364890456635\n",
      "  (0, 17)\t0.09667364890456635\n",
      "  (0, 33)\t0.09667364890456635\n",
      "  (0, 6)\t0.09667364890456635\n",
      "  (0, 2)\t0.1933472978091327\n",
      "  (0, 30)\t0.09667364890456635\n",
      "  (0, 38)\t0.1933472978091327\n",
      "  (0, 9)\t0.09667364890456635\n",
      "  (0, 28)\t0.09667364890456635\n",
      "  (0, 19)\t0.09667364890456635\n",
      "  (0, 11)\t0.09667364890456635\n",
      "  (0, 12)\t0.09667364890456635\n",
      "  (0, 31)\t0.09667364890456635\n",
      "  (0, 20)\t0.09667364890456635\n",
      "  (0, 22)\t0.09667364890456635\n",
      "  (0, 1)\t0.09667364890456635\n",
      "  (0, 39)\t0.3866945956182654\n",
      "  (0, 13)\t0.09667364890456635\n",
      "  (0, 32)\t0.09667364890456635\n",
      "  (0, 8)\t0.09667364890456635\n",
      "  (0, 4)\t0.09667364890456635\n",
      "  (0, 16)\t0.3866945956182654\n",
      "  (0, 21)\t0.09667364890456635\n",
      "  (0, 34)\t0.29002094671369905\n",
      "  (0, 14)\t0.1933472978091327\n",
      "  (0, 37)\t0.09667364890456635\n",
      "\n",
      "TF-IDF values:\n",
      "[[0.09667365 0.09667365 0.1933473  0.1933473  0.09667365 0.09667365\n",
      "  0.09667365 0.09667365 0.09667365 0.09667365 0.1933473  0.09667365\n",
      "  0.09667365 0.09667365 0.1933473  0.09667365 0.3866946  0.09667365\n",
      "  0.1933473  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
      "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.1933473\n",
      "  0.09667365 0.09667365 0.09667365 0.09667365 0.29002095 0.09667365\n",
      "  0.29002095 0.09667365 0.1933473  0.3866946 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [sample_document]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Feature names:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X)\n",
    "print(\"\\nTF-IDF values:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd56252-3033-42c3-89d4-bb876f0441f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
